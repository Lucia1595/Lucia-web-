{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "598fb1d4",
   "metadata": {},
   "source": [
    "# Ad3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d146215",
   "metadata": {},
   "source": [
    "Esta es la **actividad dirigida 3**, consiste en hacer un ejercicio de programación literaria aprovechando el código que hemos usado en programación con Python y dónde realizamos web scrapping. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acb86c18",
   "metadata": {},
   "source": [
    "# Programación literaria"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6f6e918",
   "metadata": {},
   "source": [
    "## Librerías y módulos que vamos a importar "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a401aacf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "import csv\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "import pandas as pd\n",
    "from termcolor import colored"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a745ec3d",
   "metadata": {},
   "source": [
    "**A continuación detallamos que hace cada módulo y librería**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d41455f",
   "metadata": {},
   "source": [
    "***Módulos***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7911da44",
   "metadata": {},
   "source": [
    " [time](https://www-programiz-com.translate.goog/python-programming/time?_x_tr_sl=en&_x_tr_tl=es&_x_tr_hl=es&_x_tr_pto=sc): Maneja tareas relacionadas con el tiempo. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5091163",
   "metadata": {},
   "source": [
    "[csv](https://www-programiz-com.translate.goog/python-programming/csv?_x_tr_sl=en&_x_tr_tl=es&_x_tr_hl=es&_x_tr_pto=sc): Facilita mucho el trabajo con archivos CSV."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54ba74c0",
   "metadata": {},
   "source": [
    "[re](https://docs.python.org/es/3/library/re.html): Proporciona operaciones de coincidencia de expresiones regulares similares a las encontradas en Perl."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b3de709",
   "metadata": {},
   "source": [
    "[os](https://docs-python-org.translate.goog/3/library/os.html?_x_tr_sl=en&_x_tr_tl=es&_x_tr_hl=es&_x_tr_pto=sc): Proporciona una forma portátil de usar la funcionalidad dependiente del sistema operativo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2b354b8",
   "metadata": {},
   "source": [
    "***Librerías***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "079ea89c",
   "metadata": {},
   "source": [
    "[requests](https://www-dataquest-io.translate.goog/blog/tutorial-an-introduction-to-python-requests-library/?_x_tr_sl=en&_x_tr_tl=es&_x_tr_hl=es&_x_tr_pto=sc): Admite el envío de información adicional a un servidor web a través de parámetros y encabezados, la codificación de las respuestas del servidor, la detección de errores y el manejo de redireccionamiento."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cef0c141",
   "metadata": {},
   "source": [
    "[bs4](https://omz--software-com.translate.goog/pythonista/docs/ios/beautifulsoup.html?_x_tr_sch=http&_x_tr_sl=en&_x_tr_tl=es&_x_tr_hl=es&_x_tr_pto=sc): Beautiful Soup es una librería que extraer datos de archivos HTML y XML. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38c295e2",
   "metadata": {},
   "source": [
    "[pandas](https://aprendeconalf.es/docencia/python/manual/pandas/): Es una librería de Python especializada en el manejo y análisis de estructuras de datos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b9cf4a7",
   "metadata": {},
   "source": [
    "[termcolor](https://pypi-org.translate.goog/project/termcolor/?_x_tr_sl=en&_x_tr_tl=es&_x_tr_hl=es&_x_tr_pto=sc):Formato de color ANSII para salida en terminal.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5ac208f",
   "metadata": {},
   "source": [
    "## Instalar librerías "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf4bff73",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install requests bs4 pandas termcolor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c30b53a0",
   "metadata": {},
   "source": [
    "##  Desgloce del proceso del programa a desarrollar "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56d8816b",
   "metadata": {},
   "outputs": [],
   "source": [
    "resultados = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3625be09",
   "metadata": {},
   "source": [
    "Un espacio en el que se añadirán los resultados del scrapping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06b3b5a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "req = requests.get(\"https://resultados.elpais.com\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d066fa23",
   "metadata": {},
   "source": [
    "Una de las operaciones más habituales con la librería requests es hacer una petición GET, ya sea para obtener el contenido de una web o para realizar una petición a un API.\n",
    "\n",
    "Para ello, simplemente tienes que invocar a la función get() indicando la URL a la que hacer la petición."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f465972",
   "metadata": {},
   "outputs": [],
   "source": [
    "if (req.status_code != 200):\n",
    " raise Exception(\"No se puede hacer Web Scraping en\"+ URL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33fcab85",
   "metadata": {},
   "source": [
    "La sentencia condicional ***if*** se usa para tomar decisiones, este evaluá básicamente una operación lógica. ***req.status_code***  su propio nombre describe son una serie de códigos de tres cifras estandarizados y que están relacionados con una serie de determinados errores que pueden suceder al introducir en nuestro navegador una dirección web. El valor 200 significa que la solicitud fue exitosa y el servidor respondió con los datos solicitados. En caso de que no se encuentre un resultado como el indicado se mostrará el mensaje \"No se puede hacer Web Scrapping\" en seguido de la URL errónea."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cc5c724",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(req.text, 'html.parser')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bef06a2",
   "metadata": {},
   "source": [
    "Permite extraer información de contenido en formato ***HTML o XML***. Para usarla, es necesario especificar un ***parser***, que es responsable de transformar un documento HTML o XML en un árbol complejo de objetos Python. Esto permite, por ejemplo, que podamos interactuar con los elementos de una página web como si estuviésemos utilizando las herramientas del desarrollador de un navegador."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5800f5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tags = soup.findAll(\"h2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8de2fe52",
   "metadata": {},
   "source": [
    "El objeto ***Tag*** se refiere a una etiqueta XML o HTML real en el documento. Si le agregamos la variable \"h2\" permite encontrar todas las etiquetas (***h2***) dentro de la página que han desarrollado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a21f7743",
   "metadata": {},
   "outputs": [],
   "source": [
    "for h2 in tags:\n",
    "    print(h2.text)\n",
    "    resultados.append(h2.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54cd6785",
   "metadata": {},
   "source": [
    "Usamos el código en este caso, ya que necesitamos que se imprima el texto de la etiqueta ***h2*** "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20f21264",
   "metadata": {},
   "source": [
    "Código fuente\n",
    "---------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30cbc800",
   "metadata": {},
   "source": [
    "A continuación pongo el **código fuente**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5537731b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "import csv\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "import pandas as pd\n",
    "from termcolor import colored\n",
    "\n",
    "resultados = []\n",
    "\n",
    "req = requests.get(\"https://resultados.elpais.com\")\n",
    "# Si el estatus code no es 200 no se puede leer la página\n",
    "if (req.status_code != 200):\n",
    " raise Exception(\"No se puede hacer Web Scraping en\"+ URL)\n",
    "soup = BeautifulSoup(req.text, 'html.parser')\n",
    "\n",
    "tags = soup.findAll(\"h2\")\n",
    "\n",
    "for h2 in tags:\n",
    "    print(h2.text)\n",
    "    resultados.append(h2.text)\n",
    "\n",
    "req2 = requests.get(\"https://elpais.com/internacional\")\n",
    "# Si el estatus code no es 200 no se puede leer la página\n",
    "if (req.status_code != 200):\n",
    " raise Exception(\"No se puede hacer Web Scraping en\"+ URL)\n",
    "soup2 = BeautifulSoup(req2.text, 'html.parser')\n",
    "\n",
    "tags = soup2.findAll(\"h2\")\n",
    "\n",
    "for h2 in tags:\n",
    "    print(h2.text)\n",
    "    resultados.append(h2.text)\n",
    "\n",
    "req3 = requests.get(\"https://elpais.com/opinion\")\n",
    "# Si el estatus code no es 200 no se puede leer la página\n",
    "if (req.status_code != 200):\n",
    " raise Exception(\"No se puede hacer Web Scraping en\"+ URL)\n",
    "soup3 = BeautifulSoup(req3.text, 'html.parser')\n",
    "\n",
    "tags = soup3.findAll(\"h2\")\n",
    "\n",
    "for h2 in tags:\n",
    "    print(h2.text)\n",
    "    resultados.append(h2.text)\n",
    "\n",
    "req4 = requests.get(\"https://elpais.com/espana/\")\n",
    "# Si el estatus code no es 200 no se puede leer la página\n",
    "if (req.status_code != 200):\n",
    " raise Exception(\"No se puede hacer Web Scraping en\"+ URL)\n",
    "soup4 = BeautifulSoup(req4.text, 'html.parser')\n",
    "\n",
    "tags = soup4.findAll(\"h2\")\n",
    "\n",
    "for h2 in tags:\n",
    "    print(h2.text)\n",
    "    resultados.append(h2.text)\n",
    "\n",
    "req5 = requests.get(\"https://elpais.com/economia/\")\n",
    "# Si el estatus code no es 200 no se puede leer la página\n",
    "if (req.status_code != 200):\n",
    " raise Exception(\"No se puede hacer Web Scraping en\"+ URL)\n",
    "soup5 = BeautifulSoup(req5.text, 'html.parser')\n",
    "\n",
    "tags = soup5.findAll(\"h2\")\n",
    "\n",
    "for h2 in tags:\n",
    "    print(h2.text)\n",
    "    resultados.append(h2.text)\n",
    "\n",
    "req6 = requests.get(\"https://elpais.com/sociedad/\")\n",
    "# Si el estatus code no es 200 no se puede leer la página\n",
    "if (req.status_code != 200):\n",
    " raise Exception(\"No se puede hacer Web Scraping en\"+ URL)\n",
    "soup6 = BeautifulSoup(req6.text, 'html.parser')\n",
    "\n",
    "tags = soup6.findAll(\"h2\")\n",
    "\n",
    "for h2 in tags:\n",
    "    print(h2.text)\n",
    "    resultados.append(h2.text)\n",
    "\n",
    "req7 = requests.get(\"https://elpais.com/educacion/\")\n",
    "# Si el estatus code no es 200 no se puede leer la página\n",
    "if (req.status_code != 200):\n",
    " raise Exception(\"No se puede hacer Web Scraping en\"+ URL)\n",
    "soup7 = BeautifulSoup(req7.text, 'html.parser')\n",
    "\n",
    "tags = soup7.findAll(\"h2\")\n",
    "\n",
    "for h2 in tags:\n",
    "    print(h2.text)\n",
    "    resultados.append(h2.text)\n",
    "\n",
    "req8 = requests.get(\"https://elpais.com/clima-y-medio-ambiente/\")\n",
    "# Si el estatus code no es 200 no se puede leer la página\n",
    "if (req.status_code != 200):\n",
    " raise Exception(\"No se puede hacer Web Scraping en\"+ URL)\n",
    "soup8 = BeautifulSoup(req8.text, 'html.parser')\n",
    "\n",
    "tags = soup8.findAll(\"h2\")\n",
    "\n",
    "for h2 in tags:\n",
    "    print(h2.text)\n",
    "    resultados.append(h2.text)\n",
    "\n",
    "req9 = requests.get(\"https://elpais.com/ciencia/\")\n",
    "# Si el estatus code no es 200 no se puede leer la página\n",
    "if (req.status_code != 200):\n",
    " raise Exception(\"No se puede hacer Web Scraping en\"+ URL)\n",
    "soup9 = BeautifulSoup(req9.text, 'html.parser')\n",
    "\n",
    "tags = soup9.findAll(\"h2\")\n",
    "\n",
    "for h2 in tags:\n",
    "    print(h2.text)\n",
    "    resultados.append(h2.text)\n",
    "\n",
    "req10 = requests.get(\"https://elpais.com/cultura/\")\n",
    "# Si el estatus code no es 200 no se puede leer la página\n",
    "if (req.status_code != 200):\n",
    " raise Exception(\"No se puede hacer Web Scraping en\"+ URL)\n",
    "soup10 = BeautifulSoup(req10.text, 'html.parser')\n",
    "\n",
    "tags = soup10.findAll(\"h2\")\n",
    "\n",
    "for h2 in tags:\n",
    "    print(h2.text)\n",
    "    resultados.append(h2.text)\n",
    "\n",
    "req11 = requests.get(\"https://elpais.com/babelia/\")\n",
    "# Si el estatus code no es 200 no se puede leer la página\n",
    "if (req.status_code != 200):\n",
    " raise Exception(\"No se puede hacer Web Scraping en\"+ URL)\n",
    "soup11 = BeautifulSoup(req11.text, 'html.parser')\n",
    "\n",
    "tags = soup11.findAll(\"h2\")\n",
    "\n",
    "for h2 in tags:\n",
    "    print(h2.text)\n",
    "    resultados.append(h2.text)\n",
    "\n",
    "req12 = requests.get(\"https://elpais.com/deportes/\")\n",
    "# Si el estatus code no es 200 no se puede leer la página\n",
    "if (req.status_code != 200):\n",
    " raise Exception(\"No se puede hacer Web Scraping en\"+ URL)\n",
    "soup12 = BeautifulSoup(req12.text, 'html.parser')\n",
    "\n",
    "tags = soup12.findAll(\"h2\")\n",
    "\n",
    "for h2 in tags:\n",
    "    print(h2.text)\n",
    "    resultados.append(h2.text)\n",
    "\n",
    "req13 = requests.get(\"https://elpais.com/tecnologia/\")\n",
    "# Si el estatus code no es 200 no se puede leer la página\n",
    "if (req.status_code != 200):\n",
    " raise Exception(\"No se puede hacer Web Scraping en\"+ URL)\n",
    "soup13 = BeautifulSoup(req13.text, 'html.parser')\n",
    "\n",
    "tags = soup13.findAll(\"h2\")\n",
    "\n",
    "for h2 in tags:\n",
    "    print(h2.text)\n",
    "    resultados.append(h2.text)\n",
    "\n",
    "req14 = requests.get(\"https://elpais.com/tecnologia/\")\n",
    "# Si el estatus code no es 200 no se puede leer la página\n",
    "if (req.status_code != 200):\n",
    " raise Exception(\"No se puede hacer Web Scraping en\"+ URL)\n",
    "soup14 = BeautifulSoup(req14.text, 'html.parser')\n",
    "\n",
    "tags = soup14.findAll(\"h2\")\n",
    "\n",
    "for h2 in tags:\n",
    "    print(h2.text)\n",
    "    resultados.append(h2.text)\n",
    "\n",
    "req15 = requests.get(\"https://elpais.com/gente/\")\n",
    "# Si el estatus code no es 200 no se puede leer la página\n",
    "if (req.status_code != 200):\n",
    " raise Exception(\"No se puede hacer Web Scraping en\"+ URL)\n",
    "soup15 = BeautifulSoup(req15.text, 'html.parser')\n",
    "\n",
    "tags = soup15.findAll(\"h2\")\n",
    "\n",
    "for h2 in tags:\n",
    "    print(h2.text)\n",
    "    resultados.append(h2.text)\n",
    "\n",
    "req16 = requests.get(\"https://elpais.com/television/\")\n",
    "# Si el estatus code no es 200 no se puede leer la página\n",
    "if (req.status_code != 200):\n",
    " raise Exception(\"No se puede hacer Web Scraping en\"+ URL)\n",
    "soup16 = BeautifulSoup(req16.text, 'html.parser')\n",
    "\n",
    "tags = soup16.findAll(\"h2\")\n",
    "\n",
    "for h2 in tags:\n",
    "    print(h2.text)\n",
    "    resultados.append(h2.text)\n",
    "\n",
    "req17 = requests.get(\"https://elpais.com/eps/\")\n",
    "# Si el estatus code no es 200 no se puede leer la página\n",
    "if (req.status_code != 200):\n",
    " raise Exception(\"No se puede hacer Web Scraping en\"+ URL)\n",
    "soup17 = BeautifulSoup(req17.text, 'html.parser')\n",
    "\n",
    "tags = soup17.findAll(\"h2\")\n",
    "\n",
    "for h2 in tags:\n",
    "    print(h2.text)\n",
    "    resultados.append(h2.text)\n",
    "\n",
    "\n",
    "os.system(\"clear\")\n",
    "\n",
    "print(colored(\"A continuación se muestran los titulares de las páginas principales del diario El País que contienen las siguientes palabras:\", 'blue', attrs=['bold']))\n",
    "print(colored(\"Feminismo\", 'green', attrs=['bold']))\n",
    "\n",
    "str_match = [s for s in resultados if \"feminismo\" in s]\n",
    "print(\"\\n\".join(str_match))\n",
    "\n",
    "print(colored(\"Igualdad\", 'green', attrs=['bold']))\n",
    "\n",
    "str_match = [s for s in resultados if \"igualdad\" in s]\n",
    "print(\"\\n\".join(str_match))\n",
    "\n",
    "print(colored(\"Mujeres\", 'green', attrs=['bold']))\n",
    "\n",
    "str_match = [s for s in resultados if \"mujeres\" in s]\n",
    "print(\"\\n\".join(str_match))\n",
    "\n",
    "print(colored(\"Mujer\", 'green', attrs=['bold']))\n",
    "\n",
    "str_match = [s for s in resultados if \"mujer\" in s]\n",
    "print(\"\\n\".join(str_match))\n",
    "\n",
    "print(colored(\"Brecha salarial\", 'green', attrs=['bold']))\n",
    "\n",
    "str_match = [s for s in resultados if \"brecha salarial\" in s]\n",
    "print(\"\\n\".join(str_match))\n",
    "\n",
    "print(colored(\"Machismo\", 'green', attrs=['bold']))\n",
    "\n",
    "str_match = [s for s in resultados if \"machismo\" in s]\n",
    "print(\"\\n\".join(str_match))\n",
    "\n",
    "print(colored(\"Violencia\", 'green', attrs=['bold']))\n",
    "\n",
    "str_match = [s for s in resultados if \"violencia\" in s]\n",
    "print(\"\\n\".join(str_match))\n",
    "\n",
    "print(colored(\"Maltrato\", 'green', attrs=['bold']))\n",
    "\n",
    "str_match = [s for s in resultados if \"maltrato\" in s]\n",
    "print(\"\\n\".join(str_match))\n",
    "\n",
    "print(colored(\"Homicidio\", 'green', attrs=['bold']))\n",
    "\n",
    "str_match = [s for s in resultados if \"homicidio\" in s]\n",
    "print(\"\\n\".join(str_match))\n",
    "\n",
    "print(colored(\"Género\", 'green', attrs=['bold']))\n",
    "\n",
    "str_match = [s for s in resultados if \"género\" in s]\n",
    "print(\"\\n\".join(str_match))\n",
    "\n",
    "print(colored(\"Asesinato\", 'green', attrs=['bold']))\n",
    "\n",
    "str_match = [s for s in resultados if \"asesinato\" in s]\n",
    "print(\"\\n\".join(str_match))\n",
    "\n",
    "print(colored(\"Sexo\", 'green', attrs=['bold']))\n",
    "\n",
    "str_match = [s for s in resultados if \"sexo\" in s]\n",
    "print(\"\\n\".join(str_match))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
